from langchain_core.messages import SystemMessage, HumanMessage

from states import ChatState
from config import PROCESSING_STAGES
from prompts import SYSTEM_PROMPTS
from utils.llm_clients import gpt_4o_with_tools
from utils.token_counter import count_tokens
from utils.logger import logger


def generate_answer(state: ChatState) -> ChatState:
    contexts = "\n".join(state["reranked_context"])
    messages = state.get("messages", [])
    logger.debug(f"[Generate] messages: {messages}")
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    logger.info(f"[Generate] ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘: {user_message.content}")
    logger.debug(f"[Generate] ì»¨í…ìŠ¤íŠ¸: {len(contexts)}ê°œ, ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬: {len(messages)}ê°œ")

    context_text = "\n".join(contexts)

    # ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt_with_context = SystemMessage(content=f"""
{SYSTEM_PROMPTS["generate_answer"]}

ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{context_text}
""")
    prompt = [system_prompt_with_context] + messages

    # í† í° ìˆ˜ ë¡œê¹…
    context_tokens = count_tokens(context_text)
    total_prompt_tokens = sum(count_tokens(getattr(msg, 'content', str(msg))) for msg in prompt)
    logger.debug(f"[Generate] ì»¨í…ìŠ¤íŠ¸ í† í°: {context_tokens}")
    logger.debug(f"[Generate] ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt_tokens}")

    response = gpt_4o_with_tools.invoke(prompt)

    # Tool í˜¸ì¶œ í™•ì¸
    tool_calls = getattr(response, 'tool_calls', None)
    if tool_calls:
        logger.info(f"[Generate] ğŸ”§ ë„êµ¬ í˜¸ì¶œ ìš”ì²­ë¨: {len(tool_calls)}ê°œ")
        for i, tool_call in enumerate(tool_calls):
            tool_name = tool_call.get('name', 'unknown')
            tool_args = tool_call.get('args', {})
            logger.debug(f"[Generate] ë„êµ¬ {i+1}: {tool_name}({tool_args})")
        logger.info(f"[Generate] Response: {response}")
        return {
            "messages": [response],
            "tool_call_count": state.get("tool_call_count", 0) + 1,
            "processing_stage": PROCESSING_STAGES["TOOLS_NEEDED"]
        }
    else:
        response_tokens = count_tokens(response.content) if response.content else 0
        logger.info("[Generate] âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
        logger.debug(f"[Generate] ë‹µë³€ ê¸¸ì´: {len(response.content)}ì ({response_tokens} í† í°)")
        logger.info(f"[Generate] Response: {response}")
        return {
            "final_answer": response.content or "",
            "messages": [response],
            "processing_stage": PROCESSING_STAGES["ANSWERED_WITH_CONTEXT"]
        }
