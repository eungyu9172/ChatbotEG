from langchain_core.messages import SystemMessage, HumanMessage

from states import ChatState
from config import PROCESSING_STAGES
from prompts import SYSTEM_PROMPTS
from utils.llm_clients import gpt_4o_with_tools
from utils.token_counter import count_tokens
from utils.logger import logger, format_messages_for_log  


def generate_answer(state: ChatState) -> ChatState:
    reranked_context = state.get("reranked_context") or []
    if not isinstance(reranked_context, list):
        reranked_context = []
    contexts = "\n".join(reranked_context)

    messages = state.get("messages", [])

    logger.debug(f"[Generate] messages: {format_messages_for_log(messages)}")

    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    logger.info(f"[Generate] ìµœì¢… ë‹µë³€ ìƒì„± ì‹œë„: {user_message.content}")
    logger.debug(f"[Generate] ì»¨í…ìŠ¤íŠ¸: {len(contexts)}ê°œ, ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬: {len(messages)}ê°œ")

    context_text = "\n".join(contexts)

    # âœ… State ì •ë³´ë¥¼ í¬í•¨í•œ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
    retrieve_results = state.get("retrieve_results") or []
    is_reranked = state.get("is_reranked", True)

    state_info = f"""
## í˜„ì¬ ìƒíƒœ ì •ë³´
- ê²€ìƒ‰ ê²°ê³¼ ì¡´ì¬: {"ì˜ˆ (" + str(len(retrieve_results)) + "ê°œ ë¬¸ì„œ)" if retrieve_results else "ì•„ë‹ˆì˜¤"}
- Rerank ì™„ë£Œ ì—¬ë¶€: {"ì˜ˆ" if is_reranked else "ì•„ë‹ˆì˜¤"}

## ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ
1. **retrieve_documents ì‚¬ìš© ì‹œì **:
   - ì‚¬ìš©ì ì§ˆë¬¸ì´ íŠ¹ì • ì»¬ë ‰ì…˜(innorules, technical_docs ë“±)ì˜ ì •ë³´ë¥¼ ìš”êµ¬í•  ë•Œ
   - ì•„ì§ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ì„ ë•Œ

2. **rerank_documents ì‚¬ìš© ì‹œì  (ìš°ì„ ìˆœìœ„)**:
   - ê²€ìƒ‰ ê²°ê³¼ê°€ ì¡´ì¬í•˜ê³  ({len(retrieve_results)}ê°œ ë¬¸ì„œ)
   - Rerankê°€ ì•„ì§ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ì„ ë•Œ (í˜„ì¬: {"ì™„ë£Œ" if is_reranked else "ë¯¸ì™„ë£Œ"})
   - âš ï¸ ì´ ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ **ë°˜ë“œì‹œ ë¨¼ì € rerank_documentsë¥¼ í˜¸ì¶œ**í•˜ì„¸ìš”

3. **ë‹µë³€ ìƒì„± ì‹œì **:
   - Rerankê°€ ì™„ë£Œë˜ì–´ ì •ì œëœ ë¬¸ì„œê°€ ìˆì„ ë•Œ
   - ë˜ëŠ” ë„êµ¬ ì—†ì´ ë‹µë³€ ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ì§ˆë¬¸ì¼ ë•Œ
"""

    system_prompt_with_context = SystemMessage(content=f"""
{SYSTEM_PROMPTS["generate_answer"]}

{state_info}

ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{context_text if context_text else "(ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"}
""")
    prompt = [system_prompt_with_context] + messages

    # í† í° ìˆ˜ ë¡œê¹…
    context_tokens = count_tokens(context_text)
    total_prompt_tokens = sum(count_tokens(getattr(msg, 'content', str(msg))) for msg in prompt)
    logger.debug(f"[Generate] ì»¨í…ìŠ¤íŠ¸ í† í°: {context_tokens}")
    logger.debug(f"[Generate] ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt_tokens}")

    response = gpt_4o_with_tools.invoke(prompt)

    # Tool í˜¸ì¶œ í™•ì¸
    tool_calls = getattr(response, 'tool_calls', None)
    if tool_calls:
        logger.info(f"[Generate] ğŸ”§ ë„êµ¬ í˜¸ì¶œ ìš”ì²­ë¨: {len(tool_calls)}ê°œ")
        for i, tool_call in enumerate(tool_calls):
            tool_name = tool_call.get('name', 'unknown')
            tool_args = tool_call.get('args', {})
            logger.debug(f"[Generate] ë„êµ¬ {i+1}: {tool_name}({tool_args})")
        logger.info(f"[Generate] Response: {format_messages_for_log([response])}")
        return {
            "messages": [response],
            "tool_call_count": state.get("tool_call_count", 0) + 1,
            "processing_stage": PROCESSING_STAGES["TOOL_ASSISTED_GENERATE"]
        }
    else:
        response_tokens = count_tokens(response.content) if response.content else 0
        logger.info("[Generate] âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
        logger.debug(f"[Generate] ë‹µë³€ ê¸¸ì´: {len(response.content)}ì ({response_tokens} í† í°)")
        logger.info(f"[Generate] Response: {format_messages_for_log([response])}")
        return {
            "final_answer": response.content or "",
            "messages": [response],
            "processing_stage": PROCESSING_STAGES["ANSWERED"]
        }
