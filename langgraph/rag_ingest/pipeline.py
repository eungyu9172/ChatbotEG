from tqdm import tqdm

from .config import IngestConfig
from .loaders import load_document
from .chunkers import chunk_text
from .store_chroma import ChromaStore
from .utils import (
    iter_document_paths,
    get_file_type,
    content_hash,
    now_iso,
    generate_document_id,
    extract_title,
    generate_chunk_id
)


def run_ingest(config: IngestConfig):
    store = ChromaStore(
        path=config.chroma_dir,
        collection=config.collection,
        embedding_model=config.model_name
    )
    print(f"ì„ë² ë”© ëª¨ë¸: {config.model_name}")

    # 1) ë¡œë“œ
    doc_paths = list(iter_document_paths(config.input_dir, extensions=[".pdf", ".txt"]))
    print(f"ë¬¸ì„œ íŒŒì¼: {len(doc_paths)}ê°œ")

    for path in tqdm(doc_paths, desc="Load Documents"):
        source_type = get_file_type(path)
        document_id = generate_document_id(path)
        title = extract_title(path)

        # ê¸°ì¡´ ë²„ì „ í™•ì¸
        existing_version = store.get_latest_version(document_id)
        new_version = existing_version + 1

        # ìƒì„±/ìˆ˜ì • ì‹œê° ì„¤ì •
        if existing_version == 0:
            created = now_iso()
            updated = created
            print(f"ğŸ“„ ì‹ ê·œ: {title} ({source_type.upper()}, {document_id[:16]}...)")
        else:
            existing_created = store.get_created_time(document_id)
            created = existing_created if existing_created else now_iso()
            updated = now_iso()
            print(f"ğŸ”„ ì—…ë°ì´íŠ¸: {title} ({source_type.upper()}) v{existing_version} â†’ v{new_version}")
            store.delete_document(document_id)

        # ë¬¸ì„œ ë¡œë“œ
        try:
            pages = load_document(path)
        except Exception as e:
            print(f"  âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        if not pages:
            print("  âš ï¸  ë¹ˆ ë¬¸ì„œ")
            continue

        print(f"  ğŸ“‘ {len(pages)}ê°œ í˜ì´ì§€ ë¡œë“œ")

        # Semantic Chunking
        try:
            chunks = chunk_text(pages)
        except Exception as e:
            print(f"  âŒ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            continue

        if not chunks:
            print("  âš ï¸  ìœ íš¨í•œ ì²­í¬ ì—†ìŒ")
            continue

        print(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        all_chunks_metadata = []
        all_chunk_ids = []
        all_chunk_texts = []

        total_chunk_count = len(chunks)

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        for chunk_idx, chunk_obj in enumerate(chunks):
            chunk_hash = content_hash(chunk_obj.text)
            page_str = chunk_obj.page_str

            chunk_id = generate_chunk_id(
                document_id,
                page_str,
                chunk_idx,
                chunk_hash
            )

            metadata = {
                "document_id": document_id,
                "title": title,
                "chunk_id": chunk_id,
                "page_str": page_str,
                "chunk_idx": chunk_idx,
                "chunk_count": total_chunk_count,
                "length": len(chunk_obj.text),
                "hash": chunk_hash,
                "version": new_version,
                "embedding_model": config.model_name,
                "source_type": source_type,
                "created": created,
                "updated": updated
            }

            all_chunk_ids.append(chunk_id)
            all_chunk_texts.append(chunk_obj.text)
            all_chunks_metadata.append(metadata)

        # ë°°ì¹˜ ì—…ì„œíŠ¸
        batch_size = config.batch_size
        for i in tqdm(
            range(0, len(all_chunk_texts), batch_size),
            desc=f"Upsert {title}",
            leave=False
        ):
            sl = slice(i, i + batch_size)
            store.upsert(
                ids=all_chunk_ids[sl],
                texts=all_chunk_texts[sl],
                metadatas=all_chunks_metadata[sl]
            )

        print(f"âœ… {title} v{new_version}: {len(all_chunk_texts)}ê°œ ì²­í¬ ë“±ë¡ ({source_type.upper()})")

    print(f"\nğŸ‰ ì´ {len(doc_paths)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
